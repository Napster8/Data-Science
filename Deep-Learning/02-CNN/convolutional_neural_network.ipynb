{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Work by: Raghavendra Tapas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "## Importing the libraries\n",
    "\n",
    "Keras is wrapper on Tensorflow which contains ImageDataGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIleuCAjoFD8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Pre-Processing the Training Set\n",
    "\n",
    "**Source:** We need to augument/transform the images, so that the machine doesn't overlearn the images.\n",
    "\n",
    "Keras Deep Learning Library: https://keras.io/api/preprocessing/\n",
    "\n",
    "* **Feature Scaling:** Each pixel takes a value between 0 to 255. So we need to divide each pixel by 255.\n",
    "* **Image Transformation:** Shear, zoom and horizontal flips are the image transformations.\n",
    "* **target_size:** Final size of the images before going into the training.\n",
    "* **batch_size:** 32 batch size in the batch gradient descent.\n",
    "* **class_mode:** In this specific case, we are trying to predict whether image is cat or dog. Therefore `binary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Adding First Convolution layer\n",
    "\n",
    "* `filters:` number of feature detectors\n",
    "* `activation:` rectifier activation function for all layers except output layers. Reduces linearity.\n",
    "* `input_shape:` 64*64 image size, 3 if RGB, 2 if black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling\n",
    "\n",
    "Pooling reduces the information to be processed and yet be able to detect the required features. We are also reducing the number of parameters.\n",
    "* `pool_size:` (2*2) size of the frame, note that we only specify width ( i.e. pool_size = 2 )\n",
    "* `strides:` the frame shifts to the right by 2 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening\n",
    "\n",
    "Flattening creates a 1 dimensional vector. Flattening of a `64*64` image matrix into a `4096 * 1` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection\n",
    "\n",
    "* units = hidden number of neurons, 128\n",
    "* activation function used: Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer\n",
    "\n",
    "* Binary classification - `sigmoid` .i.e. Cat or Dog?\n",
    "* multi-class classification - softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN\n",
    "\n",
    "* `Adam optimizer:` Adaptive Moment Estimation or Adam optimizer combines two gradient descent methodologies. Adaptive Moment Estimation is an algorithm for optimization technique for gradient descent. The method is really efficient when working with large problem involving a lot of data or parameters.\n",
    "\n",
    "* `Binary Cross-Entropy:` Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. Since weâ€™re trying to compute a loss, we need to penalize bad predictions. If the probability associated with the true class is 1.0, we need its loss to be zero. Conversely, if that probability is low, say, 0.01, we need its loss to be huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set\n",
    "\n",
    "* x = training set\n",
    "* validation_data = test_set\n",
    "* epochs = `25` After trying 10, 15, 20... the accuracies were not up to the par. So 25 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 370s 1s/step - loss: 2.0391 - accuracy: 6.2500e-04 - val_loss: 1.8876 - val_accuracy: 5.0000e-04\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 80s 322ms/step - loss: 1.5692 - accuracy: 0.0146 - val_loss: 1.4550 - val_accuracy: 0.0830\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 1.3618 - accuracy: 0.0056 - val_loss: 1.2811 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 81s 326ms/step - loss: 1.0260 - accuracy: 0.0019 - val_loss: 0.9740 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 82s 328ms/step - loss: 1.0070 - accuracy: 0.0000e+00 - val_loss: 1.0285 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 85s 341ms/step - loss: 0.9786 - accuracy: 0.0012 - val_loss: 0.9056 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 75s 299ms/step - loss: 0.9135 - accuracy: 5.0000e-04 - val_loss: 0.9512 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 69s 275ms/step - loss: 0.9050 - accuracy: 5.0000e-04 - val_loss: 0.9492 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.8848 - accuracy: 7.5000e-04 - val_loss: 0.8684 - val_accuracy: 0.0015\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 80s 321ms/step - loss: 0.8411 - accuracy: 0.0103 - val_loss: 0.7956 - val_accuracy: 0.0210\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 94s 374ms/step - loss: 0.7724 - accuracy: 0.0077 - val_loss: 0.7898 - val_accuracy: 0.0015\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.7534 - accuracy: 0.0046 - val_loss: 0.6835 - val_accuracy: 0.0035\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 57s 227ms/step - loss: 0.6960 - accuracy: 0.0020 - val_loss: 0.7164 - val_accuracy: 0.0040\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.7236 - accuracy: 0.0024 - val_loss: 0.7092 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.7258 - accuracy: 0.0191 - val_loss: 0.7190 - val_accuracy: 5.0000e-04\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 59s 236ms/step - loss: 0.7545 - accuracy: 0.0096 - val_loss: 0.7104 - val_accuracy: 0.0050\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.7163 - accuracy: 0.0043 - val_loss: 0.7162 - val_accuracy: 0.0205\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 57s 226ms/step - loss: 0.6989 - accuracy: 0.0063 - val_loss: 0.7361 - val_accuracy: 0.0035\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.6815 - accuracy: 0.0060 - val_loss: 0.7391 - val_accuracy: 0.0015\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.6895 - accuracy: 0.0045 - val_loss: 0.6723 - val_accuracy: 0.0070\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.6630 - accuracy: 0.0039 - val_loss: 0.6776 - val_accuracy: 0.0140\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 58s 230ms/step - loss: 0.6666 - accuracy: 0.0035 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 58s 233ms/step - loss: 0.6401 - accuracy: 0.0019 - val_loss: 0.6657 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.6790 - accuracy: 0.0030 - val_loss: 0.7189 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 59s 238ms/step - loss: 0.6916 - accuracy: 2.5000e-04 - val_loss: 0.6885 - val_accuracy: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x230c09eacd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 25/25\n",
    "250/250 [==============================] - 59s 238ms/step - loss: 0.6916 - accuracy: 2.5000e-04 - val_loss: 0.6885 - val_accuracy: 5.0000e-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction\n",
    "\n",
    "* `numpy:` to batch the test image.\n",
    "* 1 --> Dog and 0 --> Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image/255.0)\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0]  > 0.5:\n",
    "  prediction = 'It\\'s a Dog'\n",
    "else:\n",
    "  prediction = 'It\\'s a Cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Dog\n",
    "\n",
    "1. cat_or_dog_1.jpg\n",
    "\n",
    "<img src=\"dataset/single_prediction/cat_or_dog_1.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ED9KB3I54c1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a Dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that takes path of image file as input and tells whether it is a cat or dog.\n",
    "def predictImage(path):\n",
    "    test_image = image.load_img(path, target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = cnn.predict(test_image/255.0)\n",
    "    training_set.class_indices\n",
    "\n",
    "    if result[0][0]  > 0.5:\n",
    "      prediction = 'It\\'s a Dog'\n",
    "    else:\n",
    "      prediction = 'It\\'s a Cat'\n",
    "\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Cat\n",
    "\n",
    "We normalize the image by dividing the image array by 255.\n",
    "\n",
    "2. cat_or_dog_2.jpg\n",
    "\n",
    "<img src=\"dataset/single_prediction/cat_or_dog_2.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a Cat\n"
     ]
    }
   ],
   "source": [
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image/255.0)\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0]  > 0.5:\n",
    "  prediction = 'It\\'s a Dog'\n",
    "else:\n",
    "  prediction = 'It\\'s a Cat'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a Cat\n"
     ]
    }
   ],
   "source": [
    "predictImage('dataset/single_prediction/cat_or_dog_2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting 3 cats just for fun\n",
    "\n",
    "2. cat_or_dog_3.jpg\n",
    "\n",
    "<img src=\"dataset/single_prediction/cat_or_dog_3.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a Cat\n"
     ]
    }
   ],
   "source": [
    "predictImage('dataset/single_prediction/cat_or_dog_3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if its a Dog and a Cat?\n",
    "\n",
    "2. cat_or_dog_4.jpg\n",
    "\n",
    "<img src=\"dataset/single_prediction/cat_or_dog_4.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a Dog\n"
     ]
    }
   ],
   "source": [
    "predictImage('dataset/single_prediction/cat_or_dog_4.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON0YxX/oky4tPbqCLnFjWD",
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
