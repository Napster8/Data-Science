{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2ee06c-70db-4469-a095-56c19b0086ec",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "In ML, cost functions are used to estimate how badly models are performing. \n",
    "\n",
    "Put simply, a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This is typically expressed as a difference or distance between the predicted value and the actual value. The cost function (you may also see this referred to as loss or error.) can be estimated by iteratively running the model to compare estimated predictions against “ground truth” — the known values of y.\n",
    "\n",
    "The objective of a ML model, therefore, is to find parameters, weights or a structure that minimises the cost function.\n",
    "\n",
    "It is a `function` that measures the performance of a model for any given data. Cost Function `quantifies the error` between predicted values and expected values and presents it in the form of a single real number.\n",
    " $$ Hypothesis: h_{\\theta}(x) = {\\theta + \\theta_{1}(x)}$$\n",
    " $$ Parameters: {\\theta_{0} , \\theta_{1}}$$\n",
    " $$ Cost Function: {J(\\theta_{0}, \\theta_{1}) = {1\\over2m} \\displaystyle\\sum_{i=1}^{m}[(h_\\theta)(x^i) - y^i]^2}$$\n",
    " $$ Goal: minimize J({\\theta_{0} , \\theta_{1}}) with respect to  \\theta_{0}, \\theta_{1}$$\n",
    "\n",
    "**Intuition Behind Gradient Descent**\n",
    "Let’s say you are playing a game where the players are at the top of a mountain, and they are asked to reach the lowest point of the mountain. Additionally, they are blindfolded. So, what approach do you think would make you reach the lake?\n",
    "\n",
    "Take a moment to think about this before you read on.\n",
    "\n",
    "The best way is to observe the ground and find where the land descends. From that position, take a step in the descending direction and iterate this process until we reach the lowest point.\n",
    "\n",
    "# Gradient Descent \n",
    "\n",
    "[Image Link](https://miro.medium.com/max/405/1*UUHvSixG7rX2EfNFTtqBDA.gif)\n",
    "\n",
    "The goal of the gradient descent algorithm is to minimize the given function (say cost function). To achieve this goal, it performs two steps iteratively:\n",
    "\n",
    "1. Compute the gradient (slope), the first order derivative of the function at that point\n",
    "2. Make a step (move) in the direction opposite to the gradient, opposite direction of slope increase from the current point by alpha times the gradient at that point\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ecdec-81bf-4abb-b049-6c5eacef22f3",
   "metadata": {},
   "source": [
    "## Plotting Gradient Descent Algorithm\n",
    "\n",
    "When we have a single parameter (theta), we can plot the dependent variable cost on the y-axis and theta on the x-axis. If there are two parameters, we can go with a 3-D plot, with cost on one axis and the two parameters (thetas) along the other two axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad5a46-f1cf-48e3-9f27-c02e31cf1d8b",
   "metadata": {},
   "source": [
    "\n",
    "![Gradient Descent](../images/gradient_descent_algo.gif \"Gradient Descent\")\n",
    "\n",
    "Source: Miro Medium: https://miro.medium.com/max/405/1*UUHvSixG7rX2EfNFTtqBDA.gif\n",
    "\n",
    "### Gradient Descent Visualization\n",
    "\n",
    "![Gradient Descent](../images/gradient_descent.jpg \"Gradient Descent\")\n",
    "\n",
    "Source: Youtuber, [codebasics](https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ)\n",
    "\n",
    "![Overshoot](../images/overshoot.png \"Overshoot\")\n",
    "\n",
    "Source: Coursera, [Andrew NG](https://www.coursera.org/learn/machine-learning)\n",
    "\n",
    "![2D static animation of Gradient Descent](../images/grad_desc_2d.png \"Overshoot\")\n",
    "\n",
    "Source: Youtuber, [codebasics](https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f69fb7-e262-4682-9170-93e9c0dc8f10",
   "metadata": {},
   "source": [
    "## Algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4de02-3bbc-4df2-af9f-65124a8aed0f",
   "metadata": {},
   "source": [
    "* Gradient Descent Algorithm\n",
    "$$\\theta_{j} = \\theta_{j} - \\alpha [ \\frac{\\partial }{\\partial \\theta_{j}} J(\\theta_{0},\\theta_{1})]  $$\n",
    "\n",
    "* `Alpha or Learning rate` – a tuning parameter in the optimization process. It decides the length of the steps.\n",
    "* Hypothesis: \n",
    "\n",
    "$$ h_{\\theta}(x) = {\\theta + \\theta_{1}(x)} $$\n",
    "\n",
    "* Parameters:\n",
    " $$ {\\theta_{0} , \\theta_{1}} $$\n",
    " \n",
    "* Cost Function: \n",
    " $$ {J(\\theta_{0}, \\theta_{1}) = {1\\over2m} \\displaystyle\\sum_{i=1}^{m}[(h_\\theta)(x^i) - y^i]^2} $$\n",
    " \n",
    "* Goal: \n",
    " $$  minimize J({\\theta_{0} , \\theta_{1}}) with respect to  \\theta_{0}, \\theta_{1} $$\n",
    "\n",
    "* Slope:\n",
    "\n",
    " $$ slope = m = m - \\alpha * [ \\frac{\\partial }{\\partial m}]  $$\n",
    "\n",
    "where,\n",
    " \n",
    " $$  [ \\frac{\\partial }{\\partial m}] = {2\\over n} * \\displaystyle\\sum_{i=1}^{n}[y_i - (mx_i + b)]} $$\n",
    " \n",
    "* Intercept:\n",
    "\n",
    " $$ slope = c = c - \\alpha * [ \\frac{\\partial }{\\partial b}]  $$\n",
    " \n",
    "where,\n",
    " \n",
    " $$ [ \\frac{\\partial }{\\partial b}] = {2\\over n} * \\displaystyle\\sum_{i=1}^{n}[y_i - (mx_i + b)]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77422888-65d0-4cc7-a641-625dddbfb04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 4.96, c 1.44, cost 89.0 iteration 0\n",
      "m 0.4991999999999983, c 0.26879999999999993, cost 71.10560000000002 iteration 1\n",
      "m 4.451584000000002, c 1.426176000000001, cost 56.8297702400001 iteration 2\n",
      "m 0.892231679999997, c 0.5012275199999995, cost 45.43965675929613 iteration 3\n",
      "m 4.041314713600002, c 1.432759910400001, cost 36.35088701894832 iteration 4\n",
      "m 1.2008760606719973, c 0.7036872622079998, cost 29.097483330142282 iteration 5\n",
      "m 3.7095643080294423, c 1.4546767911321612, cost 23.307872849944438 iteration 6\n",
      "m 1.4424862661541864, c 0.881337636696883, cost 18.685758762535738 iteration 7\n",
      "m 3.4406683721083144, c 1.4879302070713722, cost 14.994867596913156 iteration 8\n",
      "m 1.6308855378034224, c 1.0383405553279617, cost 12.046787238456794 iteration 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Gradient Descent algorithm function\n",
    "# m = slope, c = intercept\n",
    "\n",
    "def gradient_descent(x,y):\n",
    "    m_curr = c_curr = 0\n",
    "    iterations = 10\n",
    "    n = len(x)\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr * x + c_curr\n",
    "        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\n",
    "        md = -(2/n)*sum(x*(y-y_predicted))\n",
    "        cd = -(2/n)*sum(y-y_predicted)\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        c_curr = c_curr - learning_rate * cd\n",
    "        print (\"m {}, c {}, cost {} iteration {}\".format(m_curr,c_curr,cost, i))\n",
    "\n",
    "# For a given values of 'x' and 'y' vectors\n",
    "# where x will be the known features or factors and y is something, we will be predicting, based on historical data.\n",
    "x = np.array([1,2,3,4,5])\n",
    "y = np.array([5,7,9,11,13])\n",
    "\n",
    "gradient_descent(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871871f-cdf0-4c2a-9b03-ea767befcc52",
   "metadata": {},
   "source": [
    "**Note:** Trial and error method, where we manually adjust or supervise the machine to learn what is the minimum cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03cb71e-15bf-45ed-9e71-eac959f62c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m 1.9783600000000003, b 0.027960000000000002, cost 5199.1, iteration 0\n",
      "m 0.20975041279999962, b 0.0030470367999999894, cost 4161.482445460163, iteration 1\n",
      "m 1.7908456142986242, b 0.025401286955264, cost 3332.2237319269248, iteration 2\n",
      "m 0.37738163667530467, b 0.005499731626422651, cost 2669.4843523161976, iteration 3\n",
      "m 1.6409848166378898, b 0.023373894401807944, cost 2139.826383775145, iteration 4\n",
      "m 0.5113514173939655, b 0.0074774305434828076, cost 1716.5264071567592, iteration 5\n",
      "m 1.5212165764726306, b 0.021771129698498662, cost 1378.2272007804495, iteration 6\n",
      "m 0.6184191426785134, b 0.009075514323270572, cost 1107.8601808918404, iteration 7\n",
      "m 1.4254981563597626, b 0.020507724625171385, cost 891.7842215178443, iteration 8\n",
      "m 0.7039868810749315, b 0.010370210797388455, cost 719.0974036421305, iteration 9\n",
      "\n",
      "Using gradient descent function: Coef 0.7039868810749315 Intercept 0.010370210797388455\n",
      "Using sklearn: Coef [1.01773624] Intercept 1.9152193111569176\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "\n",
    "def predict_using_sklean():\n",
    "    df = pd.read_csv(\"../data/test_scores.csv\")\n",
    "    model = LinearRegression()\n",
    "    # model.fit(x, y)\n",
    "    model.fit(df[['math']],df.cs)\n",
    "    return model.coef_, model.intercept_\n",
    "\n",
    "def gradient_descent(x,y):\n",
    "    m_curr = 0\n",
    "    b_curr = 0\n",
    "    iterations = 10\n",
    "    n = len(x)\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    cost_previous = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        cost = (1/n)*sum([value**2 for value in (y-y_predicted)])\n",
    "        md = -(2/n)*sum(x*(y-y_predicted))\n",
    "        bd = -(2/n)*sum(y-y_predicted)\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        if math.isclose(cost, cost_previous, rel_tol=1e-20):\n",
    "            break\n",
    "        cost_previous = cost\n",
    "        print (\"m {}, b {}, cost {}, iteration {}\".format(m_curr,b_curr,cost, i))\n",
    "\n",
    "    return m_curr, b_curr\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"../data/test_scores.csv\")\n",
    "    x = np.array(df.math)\n",
    "    y = np.array(df.cs)\n",
    "\n",
    "    m, b = gradient_descent(x,y)\n",
    "    print('')\n",
    "    print(\"Using gradient descent function: Coef {} Intercept {}\".format(m, b))\n",
    "\n",
    "    m_sklearn, b_sklearn = predict_using_sklean()\n",
    "    print(\"Using sklearn: Coef {} Intercept {}\".format(m_sklearn,b_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b125bcd-b4c6-456f-8594-c0ef3f123f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2f3fe5e-b736-4f34-8cd5-173a0a2fa5dc",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Difference between `Linear Regression` and `Logistic Regression`?\n",
    "\n",
    "| Linear Regression | Logistic Regression |\n",
    "| --- | --- |\n",
    "| Requires well-labeled data meaning it needs `supervision`. | Requires well-labeled data meaning it needs `supervision`. |\n",
    "| The prediction gained is usually a value that can be in the range of negative infinity to positive infinity.  | The prediction that is gained through the logistic regression is actually in the range of just zero to one. This feature allows for an easy classification with the help of a threshold value. |\n",
    "| Linear regression requires `no function of activation`. | Here we need a function of activation. In this case, that function is the `sigmoid function`. |\n",
    "| There is `no threshold value` in linear regression. | A `threshold value is needed` to determine the classes of each instance properly. |\n",
    " \n",
    "* Goal of logistic regression, is to figure out some way to split the datapoints to have an accurate prediction of a given observation class using the information present in the features.`Decision Boundary` splits the data into two parts. \n",
    "* Gradient Descent = same equation but the hypothesis function is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d02f3-0db7-4788-9c14-0efc39e5321e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
